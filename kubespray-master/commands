
git config --global user.name "Vitale Mazo"
git config --global user.email vitalemazo@gmail.

sudo apt update
sudo apt install python3 python3-pip



sudo pip3 install ansible==5.7.1 ansible-core==2.12.5 cryptography==3.4.8 jinja2==3.1.2 jmespath==1.0.1 MarkupSafe==2.1.2 netaddr==0.8.0 pbr==5.11.1 ruamel.yaml==0.17.21 ruamel.yaml.clib==0.2.7


#Change ownership of the directory: If you're sure that the directory in question should be writable by your user, you can change its ownership using the chown command. Replace username with your actual username:
sudo chown -R username /home/ghost/Documents/Kubernetes/kubespray-master/myenv/



sudo apt install python3.11-venvy
sudo python3 -m venv my
source myenv/bin/activate
pip install -r requirements.txt

#do this inside the virtual Pythong env
vagrant up
vagrant provision
vagrant up --provider=virtualbox


sudo rm -rf /Applications/Vagrant
sudo rm -f /usr/local/bin/vagrant
sudo rm -rf ~/.vagrant.d

wget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/hashicorp.list
sudo apt update && sudo apt install vagrant












 git config --list --show-origin






sudo apt update

sudo apt install git python3 python3-pip kube-system
git clone https://github.com/kubernetes-incubator/kubespray.git

vagrant up

vagrant provision

vagrant destroy



cp -rfp inventory/sample inventory/mycluster 

declare -a IPS=(192.168.56.121 192.168.56.122 192.168.56.123 192.168.56.124 192.168.56.125)


CONFIG_FILE=inventory/mycluster/hosts.yml python3 contrib/inventory_builder/inventory.py ${IPS[@]}


vi inventory/mycluster/hosts.yml


vi inventory/mycluster/group_vars/k8s-cluster/k8s-cluster.yml

vi inventory/mycluster/group_vars/k8s_cluster/addons.yml

vi inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml

nano inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml


ansible-playbook -i inventory/mycluster/hosts.yml --become --become-user=root cluster.yml


ansible-playbook -i inventory/mycluster/hosts.yaml  --become --become-user=root scale.yml

ansible-playbook -i inventory/mycluster/hosts.yaml  --become --become-user=root reset.yml


sudo su -

kubectl get pod --all-namespaces -o wide
kubectl describe pod nginx-proxy-k8s-3 -n kube-system

kubectl get nodes
kubectl describe node k8s-1

kubectl get pods --all-namespaces | grep prometheus
helm install prometheus stable/prometheus --namespace prometheus --set alertmanager.persistentVolume.storageClass="manual",server.persistentVolume.storageClass="manual"

#gets you: "nodeName namespace pod" across the cluster
kubectl get pods --all-namespaces --output 'jsonpath={range .items[*]}{.spec.nodeName}{" "}{.metadata.namespace}{" "}{.metadata.name}{"\n"}{end}'


helm repo add prometheus-community https://prometheus-community.github.io/helm-charts


kubectl get pv
kubectl get pvc --all-namespaces
kubectl get storageclasses

sudo kubectl config current-context
sudo kubectl config get-contexts

sudo kubectl config view --raw

#make local Dir on your host copy the config from host then change the IP address to Node IP
mkdir -p ~/.kube
touch ~/.kube/config
vi ~/.kube/config


cat ~/.kube/config

cat ~/.kube/config | xclip -selection clipboard
cat ~/.kube/config | xclip -selection clipboard



kubectl config view

kubectl config view --short

kubectl cluster-info

kubectl config view -o jsonpath='{.users[?(@.name == "e2e")].user.password}'